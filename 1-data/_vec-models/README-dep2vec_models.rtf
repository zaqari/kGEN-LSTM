{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf830
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 README\
\
The dep2vec files were trained off of ~200k (262,212) sentences parsed from a recent Wikimedia data dump, thoughtfully pre-processed by Omer Levy and his wonderful team at the University of Washington, in order to remove al xml elements aside from article text. While the Wikimedia file belongs to Levy and his team and I cannot in good conscience distribute that file, the dep2vec models used in this study are currently stored on a Google drive and I can provide you with on request.\
\
The model used in this algorithm includes 300D word vector representations generated from the head functors (as defined in Construction Grammar and/or HPSG) in each sentence. The number of examples used is 1 for the results in this algorithm. \
\
}